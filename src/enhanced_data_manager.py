"""
å¢å¼ºçš„æ•°æ®ç®¡ç†æ¨¡å—
Enhanced Data Manager

è´Ÿè´£Kçº¿æ•°æ®çš„è·å–ã€å¤„ç†ã€å­˜å‚¨å’Œæ–‡ä»¶ç®¡ç†
ä¿®æ”¹ä¸ºä½¿ç”¨çŸ­æ–‡ä»¶åå’Œè¦†ç›–æ¨¡å¼
"""

import os
import pandas as pd
import json
import concurrent.futures
from datetime import datetime, timezone
from pathlib import Path
from dotenv import load_dotenv
from src.logger import setup_logger
from src.config_loader import ConfigLoader
from src.account_fetcher import AccountFetcher
from src.data_fetcher import DataFetcher
from src.enhanced_technical_indicator import EnhancedTechnicalIndicator

logger = setup_logger()

class EnhancedDataManager:
    def __init__(self, config_path="config/enhanced_config.yaml"):
        """
        åˆå§‹åŒ–å¢å¼ºæ•°æ®ç®¡ç†å™¨
        
        Args:
            config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_loader = ConfigLoader(config_path)
        self.config = self.config_loader.load_config()
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # è·å–APIå‡­è¯
        self.api_key = os.getenv("OKX_API_KEY")
        self.secret_key = os.getenv("OKX_API_SECRET")
        self.passphrase = os.getenv("OKX_API_PASSPHRASE")
        self.trading_mode = os.getenv("TRADING_MODE", "1")
        self.trading_symbol = os.getenv("TRADING_SYMBOL", "BTC-USD-SWAP")
        
        if not all([self.api_key, self.secret_key, self.passphrase]):
            raise ValueError("API credentials missing. Please set OKX_API_KEY, OKX_API_SECRET, and OKX_API_PASSPHRASE")
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        self.account_fetcher = AccountFetcher(
            self.api_key, 
            self.secret_key, 
            self.passphrase,
            flag=self.trading_mode
        )
        self.data_fetcher = DataFetcher(self.api_key, self.secret_key, self.passphrase)
        
        # åˆå§‹åŒ–æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å™¨
        self.indicator_calculator = EnhancedTechnicalIndicator(self.config.get('indicators', {}))
        
        # åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•ç»“æ„
        self.base_directory = self.config.get('storage', {}).get('base_directory', 'kline_data')
        self._create_directory_structure()
        
        logger.info("Enhanced Data Manager initialized successfully")
    
    def normalize_timeframe(self, timeframe):
        """å°†æ—¶é—´å‘¨æœŸæ ‡å‡†åŒ–ä¸ºå°å†™çŸ­æ ¼å¼ï¼ˆç”¨äºç›®å½•å‘½åï¼‰"""
        return timeframe.lower()
    
    def get_okx_timeframe_format(self, timeframe):
        """å°†æ—¶é—´å‘¨æœŸè½¬æ¢ä¸ºOKX APIæ¥å—çš„æ ¼å¼"""
        # æ—¶é—´å‘¨æœŸæ˜ å°„ï¼šé…ç½®æ–‡ä»¶æ ¼å¼ -> OKX APIæ ¼å¼
        mapping = {
            '1m': '1m',    # åˆ†é’Ÿçº§åˆ«ä¿æŒä¸å˜
            '5m': '5m',
            '15m': '15m', 
            '30m': '30m',
            '1h': '1H',    # å°æ—¶çº§åˆ«éœ€è¦å¤§å†™
            '2h': '2H',
            '4h': '4H', 
            '6h': '6H',
            '12h': '12H',
            '1d': '1D',    # å¤©çº§åˆ«éœ€è¦å¤§å†™
            '3d': '3D',
            '1w': '1W'     # å‘¨çº§åˆ«éœ€è¦å¤§å†™
        }
        return mapping.get(timeframe.lower(), timeframe)
    
    def _create_directory_structure(self):
        """åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•ç»“æ„"""
        try:
            base_path = Path(self.base_directory)
            base_path.mkdir(exist_ok=True)
            
            # ä¸ºæ¯ä¸ªæ—¶é—´å‘¨æœŸåˆ›å»ºç›®å½•ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åç§°ï¼‰
            all_timeframes = []
            for category, tfs in self.config.get('timeframes', {}).items():
                all_timeframes.extend(tfs)
            
            for tf in all_timeframes:
                normalized_tf = self.normalize_timeframe(tf)
                tf_path = base_path / normalized_tf
                tf_path.mkdir(exist_ok=True)
            
            logger.info("Directory structure created successfully")
        except Exception as e:
            logger.error(f"Error creating directory structure: {e}")
    
    def _atomic_write(self, file_path, write_function):
        """åŸå­æ€§å†™å…¥æ–‡ä»¶"""
        temp_path = str(file_path) + ".tmp"
        try:
            write_function(temp_path)
            os.replace(temp_path, file_path)
        except Exception as e:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise e
    
    def get_category_for_timeframe(self, timeframe):
        """è·å–æ—¶é—´å‘¨æœŸå¯¹åº”çš„ç±»åˆ«"""
        for category, tfs in self.config.get('timeframes', {}).items():
            if timeframe in tfs:
                return category
        return 'medium_term'  # é»˜è®¤ç±»åˆ«
    
    def fetch_and_process_kline_data(self, symbol=None, timeframes=None):
        """
        è·å–å¹¶å¤„ç†Kçº¿æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¹¶å‘å¤„ç†å’Œå®æ—¶è¿›åº¦æ˜¾ç¤º
        
        Args:
            symbol (str): äº¤æ˜“å¯¹ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„
            timeframes (list): æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ‰€æœ‰
        
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        if symbol is None:
            symbol = self.trading_symbol
        
        if timeframes is None:
            timeframes = []
            for category, tfs in self.config.get('timeframes', {}).items():
                timeframes.extend(tfs)
        
        results = {
            'success': [],
            'failed': [],
            'metadata': {
                'symbol': symbol,
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'total_timeframes': len(timeframes)
            }
        }
        
        logger.info(f"ğŸš€ å¼€å§‹è·å– {symbol} çš„ {len(timeframes)} ä¸ªæ—¶é—´å‘¨æœŸæ•°æ®...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ä»¥æé«˜æ•ˆç‡
        max_workers = min(4, len(timeframes))  # é™åˆ¶å¹¶å‘æ•°ä»¥é¿å…APIé™åˆ¶
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_timeframe = {
                executor.submit(self._process_single_timeframe, symbol, tf): tf 
                for tf in timeframes
            }
            
            # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            completed = 0
            for future in concurrent.futures.as_completed(future_to_timeframe):
                timeframe = future_to_timeframe[future]
                completed += 1
                
                try:
                    result = future.result(timeout=60)  # æ¯ä¸ªæ—¶é—´å‘¨æœŸæœ€å¤šç­‰å¾…60ç§’
                    if result['success']:
                        results['success'].append(result['data'])
                        logger.info(f"[{completed}/{len(timeframes)}] {timeframe} processing completed ({result['data']['records_count']} records)")
                    else:
                        results['failed'].append({
                            'timeframe': timeframe,
                            'reason': result['error']
                        })
                        logger.warning(f"[{completed}/{len(timeframes)}] {timeframe} processing failed: {result['error']}")
                        
                except concurrent.futures.TimeoutError:
                    logger.error(f"[{completed}/{len(timeframes)}] {timeframe} processing timeout")
                    results['failed'].append({
                        'timeframe': timeframe,
                        'reason': 'Processing timeout (60s)'
                    })
                except Exception as e:
                    logger.error(f"[{completed}/{len(timeframes)}] {timeframe} processing exception: {e}")
                    results['failed'].append({
                        'timeframe': timeframe,
                        'reason': str(e)
                    })
        
        # å¤„ç†ç»“æœæ±‡æ€»
        processed_timeframes = [item['timeframe'] for item in results['success']]
        processed_normalized = [self.normalize_timeframe(tf) for tf in processed_timeframes]
        
        # æ¸…ç†æœªè·å–çš„æ—¶é—´å‘¨æœŸæ•°æ®
        self.cleanup_unused_timeframes(processed_normalized)
        
        # æ›´æ–°MCPæ¸…å•
        self.update_mcp_manifest(processed_normalized)
        
        # è‡ªåŠ¨æˆæƒæ–°ç”Ÿæˆçš„æ–‡ä»¶ç»™MCPæœåŠ¡
        self._authorize_new_files(results)
        
        success_count = len(results['success'])
        failed_count = len(results['failed'])
        logger.info(f"Data fetching completed: success {success_count}, failed {failed_count}")
        
        return results
    
    def _process_single_timeframe(self, symbol, timeframe):
        """
        å¤„ç†å•ä¸ªæ—¶é—´å‘¨æœŸçš„æ•°æ®è·å–ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        Args:
            symbol (str): äº¤æ˜“å¯¹
            timeframe (str): æ—¶é—´å‘¨æœŸ
        
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            # è·å–é…ç½®
            kline_config = self.config.get('kline_config', {}).get(timeframe, {})
            fetch_count = kline_config.get('fetch_count', 100)
            output_count = kline_config.get('output_count', 50)
            
            # è·å–Kçº¿æ•°æ®
            kline_data = self._fetch_single_timeframe_data(
                symbol, timeframe, fetch_count, output_count
            )
            
            if kline_data.empty:
                return {
                    'success': False,
                    'error': 'No data received',
                    'timeframe': timeframe
                }
            
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            category = self.get_category_for_timeframe(timeframe)
            kline_data_with_indicators = self.indicator_calculator.calculate_all_indicators(
                kline_data.copy(), category
            )
            
            # æ·»åŠ ä¿¡å·åˆ†æ
            kline_data_with_indicators = self.indicator_calculator.add_signal_analysis(
                kline_data_with_indicators
            )
            
            # ä¿å­˜æ•°æ®
            save_result = self._save_data(
                kline_data, kline_data_with_indicators, 
                symbol, timeframe
            )
            
            return {
                'success': True,
                'data': {
                    'timeframe': timeframe,
                    'records_count': len(kline_data_with_indicators),
                    'file_paths': save_result,
                    'category': category
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'timeframe': timeframe
            }
    
    def _authorize_new_files(self, fetch_results):
        """è‡ªåŠ¨æˆæƒæ–°æ–‡ä»¶ç»™MCPæœåŠ¡"""
        try:
            from src.mcp_service import load_manifest, save_manifest
            
            manifest = load_manifest()
            current_files = set(manifest.get('files', []))
            new_files = []
            
            # æ”¶é›†æ‰€æœ‰æ–°ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
            for success_item in fetch_results.get('success', []):
                file_paths = success_item.get('file_paths', {})
                for file_type, file_path in file_paths.items():
                    if file_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                        try:
                            relative_path = str(Path(file_path).relative_to(Path("kline_data")))
                            if relative_path not in current_files:
                                new_files.append(relative_path)
                                current_files.add(relative_path)
                        except ValueError:
                            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼Œè·³è¿‡
                            continue
            
            if new_files:
                manifest['files'] = sorted(list(current_files))
                save_manifest(manifest)
                logger.info(f"Auto-authorized {len(new_files)} new files for MCP access")
            
        except Exception as e:
            logger.warning(f"Failed to auto-authorize files: {e}")
    
    def _fetch_single_timeframe_data(self, symbol, timeframe, fetch_count, output_count):
        """
        è·å–å•ä¸ªæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®
        """
        try:
            # å°†æ—¶é—´å‘¨æœŸè½¬æ¢ä¸ºOKX APIæ ¼å¼
            okx_timeframe = self.get_okx_timeframe_format(timeframe)
            logger.info(f"Using OKX timeframe format: {timeframe} -> {okx_timeframe}")
            
            # è·å–å†å²Kçº¿æ•°æ®
            kline_df = self.data_fetcher.fetch_kline_data(
                instrument_id=symbol,
                bar=okx_timeframe,
                is_mark_price=False,
                limit=fetch_count
            )
            
            if kline_df.empty:
                return pd.DataFrame()
            
            # è·å–å½“å‰æœªå®Œç»“Kçº¿ï¼ˆä½¿ç”¨OKXæ ¼å¼ï¼‰
            try:
                current_kline_df = self.data_fetcher.get_current_kline(symbol, okx_timeframe)
                if not current_kline_df.empty:
                    kline_df = pd.concat([kline_df, current_kline_df])
            except Exception as e:
                logger.warning(f"Could not get current kline for {timeframe}: {e}")
            
            # æ•°æ®æ¸…ç†å’Œæ’åº
            kline_df = kline_df.sort_values('timestamp', ascending=False)
            kline_df = kline_df.head(fetch_count).sort_values('timestamp')
            kline_df = kline_df.reset_index(drop=True)
            
            # ç¡®ä¿æœ‰æˆäº¤é‡åˆ—
            if 'volume' not in kline_df.columns:
                kline_df['volume'] = 0
            
            # åªä¿ç•™æœ€è¿‘çš„æŒ‡å®šæ•°é‡
            if len(kline_df) > output_count:
                kline_df = kline_df.tail(output_count)
            
            # è®¾ç½®Kçº¿çŠ¶æ€
            kline_df["is_closed"] = True
            if len(kline_df) > 0:
                kline_df.iloc[-1, kline_df.columns.get_loc("is_closed")] = False
            
            return kline_df
            
        except Exception as e:
            logger.error(f"Error fetching {timeframe} data: {e}")
            return pd.DataFrame()
    
    def _save_data(self, kline_data, indicator_data, symbol, timeframe):
        """
        ä¿å­˜Kçº¿æ•°æ®å’ŒæŒ‡æ ‡æ•°æ®åˆ°æ–‡ä»¶ï¼ˆä½¿ç”¨çŸ­æ–‡ä»¶åï¼‰
        
        Args:
            kline_data (pd.DataFrame): åŸå§‹Kçº¿æ•°æ®
            indicator_data (pd.DataFrame): åŒ…å«æŒ‡æ ‡çš„æ•°æ®
            symbol (str): äº¤æ˜“å¯¹
            timeframe (str): æ—¶é—´å‘¨æœŸ
        
        Returns:
            dict: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        """
        try:
            normalized_tf = self.normalize_timeframe(timeframe)
            base_path = Path(self.base_directory) / normalized_tf
            base_path.mkdir(exist_ok=True)
            
            file_paths = {}
            
            # ç®€å•æ–‡ä»¶åï¼šç›´æ¥ä½¿ç”¨æ—¶é—´å‘¨æœŸåç§°
            combined_path = base_path / f"{normalized_tf}.csv"
            
            # åŸå­æ€§ä¿å­˜åˆå¹¶æ•°æ®ï¼ˆKçº¿+æŒ‡æ ‡ï¼‰
            def write_combined(temp_path):
                indicator_data.to_csv(temp_path, index=False)
            
            self._atomic_write(combined_path, write_combined)
            file_paths['combined'] = f"{normalized_tf}/{normalized_tf}.csv"
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'normalized_timeframe': normalized_tf,
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'record_count': len(indicator_data),
                'columns': list(indicator_data.columns),
                'file_paths': file_paths
            }
            
            metadata_path = base_path / "metadata.json"
            def write_metadata(temp_path):
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self._atomic_write(metadata_path, write_metadata)
            file_paths['metadata'] = f"{normalized_tf}/metadata.json"
            
            logger.info(f"Saved {timeframe} data to {combined_path}")
            return file_paths
            
        except Exception as e:
            logger.error(f"Error saving data for {timeframe}: {e}")
            return {}
    
    def cleanup_unused_timeframes(self, processed_timeframes):
        """
        æ¸…ç†æœ¬æ¬¡æœªè·å–çš„æ—¶é—´å‘¨æœŸæ•°æ®
        
        Args:
            processed_timeframes (list): æœ¬æ¬¡å¤„ç†çš„æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
        """
        try:
            base_path = Path(self.base_directory)
            if not base_path.exists():
                return
            
            deleted_files = []
            
            # éå†æ‰€æœ‰æ—¶é—´å‘¨æœŸç›®å½•
            for tf_dir in base_path.iterdir():
                if tf_dir.is_dir() and tf_dir.name not in ['logs', 'backup']:
                    tf_name = tf_dir.name
                    
                    # å¦‚æœè¿™ä¸ªæ—¶é—´å‘¨æœŸæœ¬æ¬¡æ²¡æœ‰å¤„ç†ï¼Œåˆ é™¤å…¶æ–‡ä»¶
                    if tf_name not in processed_timeframes:
                        for file_pattern in [f"{tf_name}.csv", "metadata.json"]:
                            file_path = tf_dir / file_pattern
                            if file_path.exists():
                                file_path.unlink()
                                deleted_files.append(str(file_path))
                        
                        # å¦‚æœç›®å½•ä¸ºç©ºï¼Œåˆ é™¤ç›®å½•
                        if not any(tf_dir.iterdir()):
                            tf_dir.rmdir()
                            deleted_files.append(str(tf_dir))
            
            if deleted_files:
                logger.info(f"Cleaned up {len(deleted_files)} unused timeframe files/directories")
            
        except Exception as e:
            logger.error(f"Error cleaning up unused timeframes: {e}")
    
    def update_mcp_manifest(self, processed_timeframes):
        """
        æ›´æ–°MCPæ¸…å•æ–‡ä»¶
        
        Args:
            processed_timeframes (list): å¤„ç†çš„æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
        """
        try:
            manifest_path = Path(self.base_directory) / "manifest.json"
            
            # æ„å»ºæ–°çš„æ–‡ä»¶åˆ—è¡¨
            files = []
            for tf in processed_timeframes:
                files.append(f"{tf}/{tf}.csv")
            
            manifest = {
                "files": sorted(files),
                "last_updated": datetime.utcnow().isoformat() + 'Z'
            }
            
            # åŸå­æ€§ä¿å­˜manifest
            def write_manifest(temp_path):
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(manifest, f, indent=2, ensure_ascii=False)
            
            self._atomic_write(manifest_path, write_manifest)
            logger.info(f"Updated MCP manifest with {len(files)} files")
            
        except Exception as e:
            logger.error(f"Error updating MCP manifest: {e}")
    
    def get_account_summary(self):
        """
        è·å–è´¦æˆ·æ‘˜è¦ä¿¡æ¯
        """
        try:
            balance_info = self.account_fetcher.get_balance()
            positions_info = self.account_fetcher.get_detailed_positions()
            
            return {
                'balance': balance_info,
                'positions': positions_info,
                'timestamp': datetime.utcnow().isoformat() + 'Z'
            }
        except Exception as e:
            logger.error(f"Error getting account summary: {e}")
            return {
                'balance': {'balance': 0, 'available_balance': 0},
                'positions': [],
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'error': str(e)
            }
    
    def get_market_summary(self, symbol=None):
        """
        è·å–å¸‚åœºæ‘˜è¦ä¿¡æ¯
        """
        if symbol is None:
            symbol = self.trading_symbol
        
        try:
            # è·å–å¸‚åœºæ•°æ®
            market_data = self.data_fetcher.fetch_ticker(symbol)
            funding_data = self.data_fetcher.fetch_funding_rate(symbol)
            
            return {
                'symbol': symbol,
                'market_data': market_data,
                'funding_data': funding_data,
                'timestamp': datetime.utcnow().isoformat() + 'Z'
            }
        except Exception as e:
            logger.error(f"Error getting market summary: {e}")
            return {
                'symbol': symbol,
                'market_data': {},
                'funding_data': {},
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'error': str(e)
            }
    
    def cleanup_old_files(self, days_to_keep=7):
        """
        æ¸…ç†æ—§æ–‡ä»¶ï¼ˆä¿ç•™åŠŸèƒ½ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼‰
        """
        try:
            import time
            cutoff_time = time.time() - (days_to_keep * 24 * 60 * 60)
            
            base_path = Path(self.base_directory)
            deleted_files = []
            
            # åªæ¸…ç†å¤‡ä»½ç›®å½•æˆ–æ—¥å¿—æ–‡ä»¶
            for pattern in ['backup/**/*', 'logs/**/*', '*.log']:
                for file_path in base_path.glob(pattern):
                    if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
                        file_path.unlink()
                        deleted_files.append(str(file_path))
            
            logger.info(f"Cleaned up {len(deleted_files)} old files")
            return deleted_files
            
        except Exception as e:
            logger.error(f"Error cleaning up old files: {e}")
            return []