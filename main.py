"""
AIäº¤æ˜“ç³»ç»Ÿä¸»å…¥å£
Main Entry Point for AI Trading System

é›†æˆæ•°æ®è·å–ã€æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€MCPæœåŠ¡å’ŒAIåˆ†æ
"""

import os
import json
import time
import asyncio
import schedule
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

from typing import Dict, Any
from src.logger import setup_logger
from src.enhanced_data_manager import EnhancedDataManager
from src.mcp_service import app as mcp_app
from src.ai_orchestrator import AIOrchestrator

logger = setup_logger()

class AITradingSystem:
    def __init__(self):
        """åˆå§‹åŒ–AIäº¤æ˜“ç³»ç»Ÿ"""
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
        required_vars = ['OKX_API_KEY', 'OKX_API_SECRET', 'OKX_API_PASSPHRASE', 'MCP_API_KEY', 'DASHSCOPE_API_KEY']
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            error_msg = f"Missing required environment variables: {', '.join(missing_vars)}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        try:
            self.data_manager = EnhancedDataManager()
            logger.info("AI Trading System initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize data manager: {e}")
            raise
        
        # åˆå§‹åŒ–æ¶ˆæ¯é˜Ÿåˆ—å’Œä»£ç†ç³»ç»Ÿ
        self._init_agent_system()
        
        # ç³»ç»ŸçŠ¶æ€
        self.is_running = False
        self.last_update = None
    
    def _init_agent_system(self):
        """åˆå§‹åŒ–AIåˆ†æç³»ç»Ÿ - ä½¿ç”¨è®¾è®¡æ–‡æ¡£è¦æ±‚çš„Orchestratoræ¨¡å¼"""
        try:
            # è·å–é…ç½®
            config = self.data_manager.config
            
            # æ£€æŸ¥AIåˆ†æç³»ç»Ÿæ˜¯å¦å¯ç”¨
            ai_analysis_config = config.get('ai_analysis', {})
            if not ai_analysis_config.get('enabled', True):
                logger.info("AI analysis system is disabled in configuration")
                self.ai_orchestrator = None
                return
            
            # åˆå§‹åŒ–AIç¼–æ’å™¨ï¼ˆæŒ‰ç…§è®¾è®¡æ–‡æ¡£çš„é¡¹ç›®ç»ç†æ¨¡å¼ï¼‰
            try:
                self.ai_orchestrator = AIOrchestrator()
                logger.info("AI Orchestrator initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize AI Orchestrator: {e}")
                self.ai_orchestrator = None
                
        except Exception as e:
            logger.error(f"Failed to initialize AI analysis system: {e}")
            self.ai_orchestrator = None
        
    def fetch_all_data(self):
        """è·å–æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„æ•°æ®"""
        try:
            logger.info("Starting comprehensive data fetch...")
            
            # è·å–è´¦æˆ·ä¿¡æ¯
            account_summary = self.data_manager.get_account_summary()
            
            # è·å–å¸‚åœºä¿¡æ¯  
            market_summary = self.data_manager.get_market_summary()
            
            # è·å–å¹¶å¤„ç†æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®
            results = self.data_manager.fetch_and_process_kline_data()
            
            # è‡ªåŠ¨æˆæƒæ–°ç”Ÿæˆçš„æ–‡ä»¶ç»™MCPæœåŠ¡
            self._authorize_new_files(results)
            
            # ä¿å­˜è¿è¡Œæ‘˜è¦
            self._save_run_summary(results, account_summary, market_summary)
            
            # è§¦å‘åˆ†æè¯·æ±‚ï¼ˆå¦‚æœä»£ç†ç³»ç»Ÿå¯ç”¨ï¼‰
            self._trigger_analysis_if_enabled(results)
            
            self.last_update = datetime.utcnow()
            logger.info("Data fetch completed successfully")
            
            return results
            
        except Exception as e:
            logger.error(f"Error in fetch_all_data: {e}")
            return {'success': [], 'failed': [], 'error': str(e)}
    
    def _authorize_new_files(self, fetch_results):
        """è‡ªåŠ¨æˆæƒæ–°æ–‡ä»¶ç»™MCPæœåŠ¡"""
        try:
            from src.mcp_service import load_manifest, save_manifest
            
            manifest = load_manifest()
            current_files = set(manifest.get('files', []))
            new_files = []
            
            # æ”¶é›†æ‰€æœ‰æ–°ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
            for success_item in fetch_results.get('success', []):
                file_paths = success_item.get('file_paths', {})
                for file_type, file_path in file_paths.items():
                    if file_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                        try:
                            relative_path = str(Path(file_path).relative_to(Path("kline_data")))
                            if relative_path not in current_files:
                                new_files.append(relative_path)
                                current_files.add(relative_path)
                        except ValueError:
                            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼Œè·³è¿‡
                            continue
            
            if new_files:
                manifest['files'] = sorted(list(current_files))
                save_manifest(manifest)
                logger.info(f"Auto-authorized {len(new_files)} new files for MCP access")
            
        except Exception as e:
            logger.warning(f"Failed to auto-authorize files: {e}")
    
    def _save_run_summary(self, fetch_results, account_summary, market_summary):
        """ä¿å­˜è¿è¡Œæ‘˜è¦"""
        try:
            summary = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'fetch_results': fetch_results,
                'account_summary': account_summary,
                'market_summary': market_summary,
                'system_status': {
                    'is_running': self.is_running,
                    'last_update': self.last_update.isoformat() + 'Z' if self.last_update else None
                }
            }
            
            summary_dir = Path('system_logs')
            summary_dir.mkdir(exist_ok=True)
            
            timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
            summary_file = summary_dir / f'run_summary_{timestamp_str}.json'
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Run summary saved to: {summary_file}")
            
        except Exception as e:
            logger.error(f"Failed to save run summary: {e}")
    
    def setup_scheduler(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        try:
            # æ¯å°æ—¶è·å–ä¸€æ¬¡æ•°æ®
            schedule.every().hour.do(self.fetch_all_data)
            
            # æ¯å¤©æ¸…ç†ä¸€æ¬¡æ—§æ–‡ä»¶
            schedule.every().day.at("02:00").do(self._cleanup_old_files)
            
            logger.info("Scheduler setup completed")
            
        except Exception as e:
            logger.error(f"Failed to setup scheduler: {e}")
    
    def _cleanup_old_files(self):
        """æ¸…ç†æ—§æ–‡ä»¶"""
        try:
            deleted_files = self.data_manager.cleanup_old_files(days_to_keep=7)
            logger.info(f"Cleanup completed: removed {len(deleted_files)} old files")
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    def _trigger_analysis_if_enabled(self, data_results: Dict[str, Any]):
        """å¦‚æœAIåˆ†æç³»ç»Ÿå¯ç”¨ï¼Œè§¦å‘åˆ†æè¯·æ±‚"""
        if not hasattr(self, 'ai_orchestrator') or not self.ai_orchestrator:
            logger.debug("AI Orchestrator not available, skipping analysis")
            return
        
        try:
            # æ„å»ºåˆ†æè¯·æ±‚
            successful_timeframes = len(data_results.get('success', []))
            failed_timeframes = len(data_results.get('failed', []))
            timeframes = [tf.get('timeframe') for tf in data_results.get('success', [])]
            
            analysis_request = f"""è¯·åˆ†ææœ€æ–°çš„å¸‚åœºæ•°æ®ã€‚
            
æ•°æ®æ›´æ–°æ‘˜è¦ï¼š
- æˆåŠŸè·å– {successful_timeframes} ä¸ªæ—¶é—´å‘¨æœŸæ•°æ®
- å¤±è´¥ {failed_timeframes} ä¸ªæ—¶é—´å‘¨æœŸ
- å¯ç”¨æ—¶é—´å‘¨æœŸï¼š{', '.join(timeframes)}
- æ›´æ–°æ—¶é—´ï¼š{datetime.utcnow().isoformat()}

è¯·åŸºäºæœ€æ–°æ•°æ®è¿›è¡Œå¸‚åœºåˆ†æï¼Œå¹¶æä¾›äº¤æ˜“å»ºè®®ã€‚"""

            # ä½¿ç”¨AIç¼–æ’å™¨æ‰§è¡Œåˆ†æ
            logger.info("Triggering AI analysis after data update")
            analysis_result = self.ai_orchestrator.analyze_market(analysis_request)
            
            if analysis_result.get('success', False):
                logger.info("AI analysis completed successfully")
                # è¿™é‡Œå¯ä»¥æ·»åŠ å¯¹åˆ†æç»“æœçš„å¤„ç†é€»è¾‘
                self._process_analysis_result(analysis_result)
            else:
                logger.error(f"AI analysis failed: {analysis_result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logger.error(f"Failed to trigger analysis: {e}")
    
    def _process_analysis_result(self, analysis_result: Dict[str, Any]):
        """å¤„ç†åˆ†æç»“æœ"""
        try:
            # è®°å½•åˆ†æç»“æœ
            analysis = analysis_result.get('analysis', 'No analysis available')
            iterations = analysis_result.get('iterations', 0)
            
            logger.info(f"AI Analysis Result (after {iterations} iterations):")
            logger.info(f"Analysis: {analysis}")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„ç»“æœå¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚ï¼š
            # - ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
            # - å‘é€é€šçŸ¥
            # - è§¦å‘äº¤æ˜“åŠ¨ä½œç­‰
            
        except Exception as e:
            logger.error(f"Error processing analysis result: {e}")
    
    def start_ai_analysis_system(self):
        """å¯åŠ¨AIåˆ†æç³»ç»Ÿ"""
        if not hasattr(self, 'ai_orchestrator') or not self.ai_orchestrator:
            logger.info("AI Orchestrator not initialized, skipping start")
            return
        
        try:
            # AIç¼–æ’å™¨ä¸éœ€è¦æ˜¾å¼å¯åŠ¨ï¼Œå®ƒæ˜¯åŸºäºè°ƒç”¨çš„æ¨¡å¼
            # éªŒè¯ç³»ç»ŸçŠ¶æ€
            status = self.ai_orchestrator.get_analysis_status()
            logger.info("AI Analysis System status verified")
            logger.info(f"Available tools: {status.get('available_tools', [])}")
            logger.info("AI Analysis System ready for use")
            
        except Exception as e:
            logger.error(f"Failed to start AI analysis system: {e}")
    
    def stop_ai_analysis_system(self):
        """åœæ­¢AIåˆ†æç³»ç»Ÿ"""
        try:
            # AIç¼–æ’å™¨ä¸éœ€è¦æ˜¾å¼åœæ­¢
            # è®°å½•åœæ­¢ä¿¡æ¯
            logger.info("AI Analysis System stopped")
            
        except Exception as e:
            logger.error(f"Failed to stop AI analysis system: {e}")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        logger.info("Starting scheduler...")
        self.is_running = True
        
        while self.is_running:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logger.info("Scheduler stopped by user")
                self.is_running = False
                break
            except Exception as e:
                logger.error(f"Scheduler error: {e}")
                time.sleep(60)
    
    def run_once(self):
        """è¿è¡Œä¸€æ¬¡æ•°æ®è·å–"""
        logger.info("Running single data fetch...")
        return self.fetch_all_data()

def run_mcp_service():
    """è¿è¡ŒMCPæœåŠ¡"""
    import uvicorn
    logger.info("Starting MCP service on port 5000...")
    
    # ç¡®ä¿MCP APIå¯†é’¥å·²è®¾ç½®
    if not os.getenv('MCP_API_KEY'):
        logger.error("MCP_API_KEY not set!")
        return
    
    try:
        uvicorn.run(
            "main:mcp_app", 
            host="0.0.0.0", 
            port=5000, 
            log_level="info",
            reload=False
        )
    except Exception as e:
        logger.error(f"MCP service error: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("AI Trading System Starting...")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        trading_system = AITradingSystem()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        trading_system.setup_scheduler()
        
        # å¯åŠ¨AIåˆ†æç³»ç»Ÿï¼ˆè®¾è®¡æ–‡æ¡£è¦æ±‚çš„Orchestratoræ¨¡å¼ï¼‰
        print("\nğŸ¤– å¯åŠ¨AIåˆ†æç³»ç»Ÿ...")
        trading_system.start_ai_analysis_system()
        
        # ç«‹å³è¿è¡Œä¸€æ¬¡æ•°æ®è·å–
        print("\nğŸ”„ æ‰§è¡Œåˆå§‹æ•°æ®è·å–...")
        initial_results = trading_system.run_once()
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        successful_timeframes = len(initial_results.get('success', []))
        failed_timeframes = len(initial_results.get('failed', []))
        print(f"âœ… æˆåŠŸå¤„ç† {successful_timeframes} ä¸ªæ—¶é—´å‘¨æœŸ")
        
        if failed_timeframes > 0:
            print(f"âŒ å¤±è´¥ {failed_timeframes} ä¸ªæ—¶é—´å‘¨æœŸ")
            for failed in initial_results.get('failed', []):
                print(f"   - {failed.get('timeframe')}: {failed.get('reason')}")
        
        # å¯åŠ¨MCPæœåŠ¡
        print("\nğŸš€ å¯åŠ¨MCPæœåŠ¡ (ç«¯å£ 5000)...")
        print("ğŸ“Š ç³»ç»Ÿå·²å°±ç»ª - AIç°åœ¨å¯ä»¥é€šè¿‡MCPè®¿é—®æ•°æ®")
        print("\n" + "=" * 60)
        print("ç³»ç»Ÿæ­£åœ¨è¿è¡Œ...")
        print("æŒ‰ Ctrl+C åœæ­¢")
        print("=" * 60)
        
        # åœ¨åå°è¿è¡Œå®šæ—¶ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=3) as executor:
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡
            scheduler_future = executor.submit(trading_system.start_scheduler)
            
            # å¯åŠ¨MCPæœåŠ¡ï¼ˆä¸»çº¿ç¨‹ï¼‰
            run_mcp_service()
        
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {e}")
        logger.error(f"System error: {e}")
    finally:
        # åœæ­¢AIåˆ†æç³»ç»Ÿ
        try:
            trading_system.stop_ai_analysis_system()
        except:
            pass
        print("\nğŸ‘‹ AIäº¤æ˜“ç³»ç»Ÿå·²åœæ­¢")

if __name__ == "__main__":
    main()
