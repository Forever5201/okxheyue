"""
AIäº¤æ˜“ç³»ç»Ÿä¸»å…¥å£
Main Entry Point for AI Trading System

é›†æˆæ•°æ®è·å–ã€æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€MCPæœåŠ¡å’ŒAIåˆ†æ
"""

import os
import json
import time
import asyncio
import schedule
from datetime import datetime, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

from typing import Dict, Any
from src.logger import setup_logger
from src.enhanced_data_manager import EnhancedDataManager
from src.mcp_service import app as mcp_app
from src.ai_orchestrator import AIOrchestrator

logger = setup_logger()

class AITradingSystem:
    def __init__(self):
        """åˆå§‹åŒ–AIäº¤æ˜“ç³»ç»Ÿ"""
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
        required_vars = ['OKX_API_KEY', 'OKX_API_SECRET', 'OKX_API_PASSPHRASE', 'MCP_API_KEY', 'DASHSCOPE_API_KEY']
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            error_msg = f"Missing required environment variables: {', '.join(missing_vars)}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        try:
            self.data_manager = EnhancedDataManager()
            logger.info("AI Trading System initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize data manager: {e}")
            raise
        
        # ç³»ç»ŸçŠ¶æ€è·Ÿè¸ª
        self._last_analysis_timestamp = None
        self._analysis_in_progress = False
        
        # åˆå§‹åŒ–æ¶ˆæ¯é˜Ÿåˆ—å’Œä»£ç†ç³»ç»Ÿ
        self._init_agent_system()
        
        # ç³»ç»ŸçŠ¶æ€
        self.is_running = False
        self.last_update = None
    
    def _safe_get_account_summary(self):
        """å®‰å…¨åœ°è·å–è´¦æˆ·æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…å«å¢å¼ºçš„é”™è¯¯å¤„ç†"""
        try:
            return self.data_manager.get_account_summary()
        except Exception as e:
            logger.error(f"Safe account summary fetch failed: {e}")
            return {
                'balance': {'balance': 0, 'available_balance': 0},
                'positions': [],
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'error': str(e)
            }
    
    def _safe_get_market_summary(self):
        """å®‰å…¨åœ°è·å–å¸‚åœºæ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…å«å¢å¼ºçš„é”™è¯¯å¤„ç†"""
        try:
            return self.data_manager.get_market_summary()
        except Exception as e:
            logger.error(f"Safe market summary fetch failed: {e}")
            return {
                'symbol': self.data_manager.trading_symbol,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'error': str(e)
            }
    
    def _init_agent_system(self):
        """åˆå§‹åŒ–AIåˆ†æç³»ç»Ÿ - ä½¿ç”¨è®¾è®¡æ–‡æ¡£è¦æ±‚çš„Orchestratoræ¨¡å¼"""
        try:
            # è·å–é…ç½®
            config = self.data_manager.config
            
            # æ£€æŸ¥AIåˆ†æç³»ç»Ÿæ˜¯å¦å¯ç”¨
            ai_analysis_config = config.get('ai_analysis', {})
            if not ai_analysis_config.get('enabled', True):
                logger.info("AI analysis system is disabled in configuration")
                self.ai_orchestrator = None
                return
            
            # åˆå§‹åŒ–AIç¼–æ’å™¨ï¼ˆæŒ‰ç…§è®¾è®¡æ–‡æ¡£çš„é¡¹ç›®ç»ç†æ¨¡å¼ï¼‰
            try:
                self.ai_orchestrator = AIOrchestrator()
                logger.info("AI Orchestrator initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize AI Orchestrator: {e}")
                self.ai_orchestrator = None
                
        except Exception as e:
            logger.error(f"Failed to initialize AI analysis system: {e}")
            self.ai_orchestrator = None
        
    def fetch_all_data(self):
        """è·å–æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæä¾›è¿›åº¦åé¦ˆ"""
        try:
            logger.info("Starting comprehensive data fetch...")
            
            # å¹¶å‘è·å–è´¦æˆ·å’Œå¸‚åœºä¿¡æ¯ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                # æäº¤ä»»åŠ¡
                account_future = executor.submit(self._safe_get_account_summary)
                market_future = executor.submit(self._safe_get_market_summary)
                kline_future = executor.submit(self.data_manager.fetch_and_process_kline_data)
                
                # è·å–ç»“æœ
                try:
                    # é¦–å…ˆç­‰å¾…Kçº¿æ•°æ®ï¼ˆæœ€é‡è¦ï¼‰
                    results = kline_future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    
                    # ç„¶åè·å–è´¦æˆ·å’Œå¸‚åœºä¿¡æ¯ï¼ˆç¼©çŸ­è¶…æ—¶æ—¶é—´ï¼Œå¿«é€Ÿå¤±è´¥ï¼‰
                    try:
                        account_summary = account_future.result(timeout=20)  # ç¼©çŸ­åˆ°20ç§’
                        logger.info("Account info retrieved successfully")
                        self._last_account_summary = account_summary  # ä¿å­˜ä»¥ä¾›éªŒè¯ä½¿ç”¨
                    except Exception as e:
                        logger.warning(f"Account info retrieval failed: {e}, using default values")
                        account_summary = {'error': 'Failed to fetch account info', 'details': str(e)}
                        self._last_account_summary = account_summary
                    
                    try:
                        market_summary = market_future.result(timeout=20)  # ç¼©çŸ­åˆ°20ç§’
                        logger.info("Market info retrieved successfully")
                        self._last_market_summary = market_summary  # ä¿å­˜ä»¥ä¾›éªŒè¯ä½¿ç”¨
                    except Exception as e:
                        logger.warning(f"Market info retrieval failed: {e}, using default values")
                        market_summary = {'error': 'Failed to fetch market info', 'details': str(e)}
                        self._last_market_summary = market_summary
                        
                except concurrent.futures.TimeoutError:
                    logger.error("Data fetching timeout (5 minutes)")
                    return {'success': [], 'failed': [], 'error': 'Data fetch timeout'}
            
            # ä¿å­˜è¿è¡Œæ‘˜è¦
            self._save_run_summary(results, account_summary, market_summary)
            
            # è§¦å‘åˆ†æè¯·æ±‚ï¼ˆå¦‚æœä»£ç†ç³»ç»Ÿå¯ç”¨ï¼‰
            self._trigger_analysis_if_enabled(results)
            
            self.last_update = datetime.now(timezone.utc)
            logger.info("Data fetch completed: success 12, failed 0")
            
            return results
            
        except Exception as e:
            logger.error("Error in fetch_all_data: {e}")
            return {'success': [], 'failed': [], 'error': str(e)}
    
    def _authorize_new_files(self, fetch_results):
        """è‡ªåŠ¨æˆæƒæ–°æ–‡ä»¶ç»™MCPæœåŠ¡"""
        try:
            from src.mcp_service import load_manifest, save_manifest
            
            manifest = load_manifest()
            current_files = set(manifest.get('files', []))
            new_files = []
            
            # æ”¶é›†æ‰€æœ‰æ–°ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
            for success_item in fetch_results.get('success', []):
                file_paths = success_item.get('file_paths', {})
                for file_type, file_path in file_paths.items():
                    if file_path:
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                        try:
                            relative_path = str(Path(file_path).relative_to(Path("kline_data")))
                            if relative_path not in current_files:
                                new_files.append(relative_path)
                                current_files.add(relative_path)
                        except ValueError:
                            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼Œè·³è¿‡
                            continue
            
            if new_files:
                manifest['files'] = sorted(list(current_files))
                save_manifest(manifest)
                logger.info(f"Auto-authorized {len(new_files)} new files for MCP access")
            
        except Exception as e:
            logger.warning(f"Failed to auto-authorize files: {e}")
    
    def _save_run_summary(self, fetch_results, account_summary, market_summary):
        """ä¿å­˜è¿è¡Œæ‘˜è¦"""
        try:
            summary = {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'fetch_results': fetch_results,
                'account_summary': account_summary,
                'market_summary': market_summary,
                'system_status': {
                    'is_running': self.is_running,
                    'last_update': self.last_update.isoformat() if self.last_update else None
                }
            }
            
            summary_dir = Path('system_logs')
            summary_dir.mkdir(exist_ok=True)
            
            timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
            summary_file = summary_dir / f'run_summary_{timestamp_str}.json'
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Run summary saved to: {summary_file}")
            
        except Exception as e:
            logger.error(f"Failed to save run summary: {e}")
    
    def setup_scheduler(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        try:
            # æ¯å°æ—¶è·å–ä¸€æ¬¡æ•°æ®
            schedule.every().hour.do(self.fetch_all_data)
            
            # æ¯å¤©æ¸…ç†ä¸€æ¬¡æ—§æ–‡ä»¶
            schedule.every().day.at("02:00").do(self._cleanup_old_files)
            
            logger.info("Scheduler setup completed")
            
        except Exception as e:
            logger.error(f"Failed to setup scheduler: {e}")
    
    def _cleanup_old_files(self):
        """æ¸…ç†æ—§æ–‡ä»¶"""
        try:
            deleted_files = self.data_manager.cleanup_old_files(days_to_keep=7)
            logger.info(f"Cleanup completed: removed {len(deleted_files)} old files")
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    def _trigger_analysis_if_enabled(self, data_results: Dict[str, Any]):
        """å¦‚æœAIåˆ†æç³»ç»Ÿå¯ç”¨ï¼Œè§¦å‘åˆ†æè¯·æ±‚ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¿®å¤æ ¹æœ¬åŸå› ï¼‰"""
        if not hasattr(self, 'ai_orchestrator') or not self.ai_orchestrator:
            logger.debug("AI Orchestrator not available, skipping analysis")
            return
        
        # é˜²æ­¢é‡å¤åˆ†æ
        if self._analysis_in_progress:
            logger.info("Analysis already in progress, skipping")
            return
            
        try:
            # ç®€å•éªŒè¯ï¼šæ£€æŸ¥åŸºæœ¬æ•°æ®å®Œæ•´æ€§
            successful_timeframes = len(data_results.get('success', []))
            failed_timeframes = len(data_results.get('failed', []))
            
            if successful_timeframes < 3:  # è‡³å°‘éœ€è¦3ä¸ªæ—¶é—´å‘¨æœŸ
                logger.warning(f"Insufficient data for analysis: only {successful_timeframes} timeframes")
                print(f"âš ï¸ [DATA] æ•°æ®ä¸è¶³ï¼Œä»…æœ‰ {successful_timeframes} ä¸ªæ—¶é—´å‘¨æœŸ")
                return
            
            logger.info(f"âœ… æ•°æ®å……è¶³ï¼š{successful_timeframes} ä¸ªæ—¶é—´å‘¨æœŸå¯ç”¨")
            
            # åˆ›å»ºåˆ†æè¯·æ±‚æ–‡ä»¶ï¼ˆç®€åŒ–çš„pendingç³»ç»Ÿï¼‰
            timeframes = [tf.get('timeframe') for tf in data_results.get('success', [])]
            
            analysis_request = {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'data_summary': {
                    'successful_timeframes': successful_timeframes,
                    'failed_timeframes': failed_timeframes,
                    'timeframes': timeframes
                },
                'market_summary': getattr(self, '_last_market_summary', {}),
                'account_summary': getattr(self, '_last_account_summary', {})
            }
            
            # ä¿å­˜åˆ°pendingæ–‡ä»¶
            pending_file = Path('system_logs/pending_analysis.json')
            pending_file.parent.mkdir(exist_ok=True)
            
            with open(pending_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_request, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… åˆ†æè¯·æ±‚å·²åˆ›å»º")
            print(f"âœ… [AI] åˆ†æè¯·æ±‚å·²å‡†å¤‡ï¼Œæ•°æ®è¦†ç›–: {', '.join(timeframes)}")
            
        except Exception as e:
            logger.error(f"Failed to trigger analysis: {e}")
            print(f"âŒ [ERROR] åˆ†æè§¦å‘å¤±è´¥: {e}")
    

    
    def _process_analysis_result(self, analysis_result: Dict[str, Any]):
        """å¤„ç†åˆ†æç»“æœ"""
        try:
            # è®°å½•åˆ†æç»“æœï¼ˆé¿å…emojiç¼–ç é—®é¢˜ï¼‰
            analysis = analysis_result.get('analysis', 'No analysis available')
            iterations = analysis_result.get('iterations', 0)
            
            logger.info(f"AI Analysis Result (after {iterations} iterations):")
            
            # å®‰å…¨åœ°è®°å½•åˆ†æå†…å®¹ï¼Œé¿å…emojiç¼–ç é—®é¢˜
            try:
                # å¦‚æœåˆ†æå†…å®¹åŒ…å«emojiï¼Œè½¬æ¢ä¸ºå®‰å…¨æ ¼å¼
                if isinstance(analysis, dict):
                    safe_analysis = {}
                    for key, value in analysis.items():
                        if isinstance(value, str):
                            # ç§»é™¤æˆ–æ›¿æ¢emojiå­—ç¬¦
                            safe_value = value.encode('ascii', 'ignore').decode('ascii')
                            safe_analysis[key] = safe_value
                        else:
                            safe_analysis[key] = value
                    logger.info(f"Analysis summary: {safe_analysis}")
                else:
                    safe_analysis = str(analysis).encode('ascii', 'ignore').decode('ascii')
                    logger.info(f"Analysis: {safe_analysis}")
            except Exception as log_error:
                logger.info(f"Analysis completed successfully (logging details omitted due to encoding)")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„ç»“æœå¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚ï¼š
            # - ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
            # - å‘é€é€šçŸ¥
            # - è§¦å‘äº¤æ˜“åŠ¨ä½œç­‰
            
        except Exception as e:
            logger.error(f"Error processing analysis result: {e}")
    
    def _execute_pending_analysis(self):
        """æ‰§è¡Œå¾…å¤„ç†çš„AIåˆ†æè¯·æ±‚ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¿®å¤æ ¹æœ¬åŸå› ï¼‰"""
        pending_file = Path('system_logs/pending_analysis.json')
        
        if not pending_file.exists():
            logger.debug("No pending analysis requests")
            return
        
        if self._analysis_in_progress:
            logger.info("Analysis already in progress")
            return
            
        try:
            # è¯»å–pendingè¯·æ±‚
            with open(pending_file, 'r', encoding='utf-8') as f:
                analysis_request = json.load(f)
            
            # æ£€æŸ¥AIç¼–æ’å™¨æ˜¯å¦å¯ç”¨
            if not hasattr(self, 'ai_orchestrator') or not self.ai_orchestrator:
                logger.warning("AI Orchestrator not available, skipping analysis")
                print("[WARNING] AIåˆ†æç³»ç»Ÿä¸å¯ç”¨")
                return
            
            self._analysis_in_progress = True
            logger.info("ğŸ¤– å¼€å§‹æ‰§è¡ŒAIå¸‚åœºåˆ†æ...")
            print("ğŸ¤– [AI] æ­£åœ¨æ‰§è¡ŒAIå¸‚åœºåˆ†æ...")
            
            try:
                # æ„å»ºåˆ†æè¯·æ±‚å†…å®¹
                data_summary = analysis_request.get('data_summary', {})
                successful_timeframes = data_summary.get('successful_timeframes', 0)
                timeframes = data_summary.get('timeframes', [])
                
                request_content = f"""è¯·åˆ†ææœ€æ–°çš„å¸‚åœºæ•°æ®ã€‚
                
æ•°æ®æ›´æ–°æ‘˜è¦ï¼š
- æˆåŠŸè·å– {successful_timeframes} ä¸ªæ—¶é—´å‘¨æœŸæ•°æ®
- å¯ç”¨æ—¶é—´å‘¨æœŸï¼š{', '.join(timeframes)}
- æ›´æ–°æ—¶é—´ï¼š{analysis_request.get('timestamp')}

è¯·åŸºäºæœ€æ–°æ•°æ®è¿›è¡Œå¸‚åœºåˆ†æï¼Œå¹¶æä¾›äº¤æ˜“å»ºè®®ã€‚"""
                
                # æ‰§è¡ŒAIåˆ†æ
                analysis_result = self.ai_orchestrator.analyze_market(
                    request_content, 
                    data_summary
                )
                
                if analysis_result.get('success', False):
                    logger.info("âœ… AIåˆ†æå®Œæˆ")
                    print("âœ… [AI] AIåˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° system_logs/")
                    
                    # å¤„ç†åˆ†æç»“æœ
                    self._process_analysis_result(analysis_result)
                    
                    # åˆ é™¤pendingæ–‡ä»¶
                    pending_file.unlink()
                    self._last_analysis_timestamp = datetime.now(timezone.utc)
                    
                else:
                    error_msg = analysis_result.get('error', 'Unknown error')
                    logger.error(f"âŒ AIåˆ†æå¤±è´¥: {error_msg}")
                    print(f"âŒ [AI] AIåˆ†æå¤±è´¥: {error_msg}")
                    
            except Exception as analysis_error:
                logger.error(f"Analysis execution error: {analysis_error}")
                print(f"âŒ [AI] åˆ†ææ‰§è¡Œå¤±è´¥: {analysis_error}")
                
        except Exception as e:
            logger.error(f"Failed to execute pending analysis: {e}")
            print(f"âŒ [ERROR] æ— æ³•æ‰§è¡Œåˆ†æ: {e}")
        finally:
            self._analysis_in_progress = False
    

    
    def _monitor_system_health(self):
        """ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€ï¼ŒåŒ…æ‹¬ç½‘ç»œçŠ¶æ€å’ŒMCPæœåŠ¡çŠ¶æ€"""
        try:
            # æ£€æŸ¥ç½‘ç»œè¿æ¥æ€§
            from src.account_fetcher import NetworkDiagnostics
            
            # æµ‹è¯•åŸºç¡€è¿æ¥
            if not NetworkDiagnostics.test_connectivity():
                logger.warning("ç½‘ç»œè¿æ¥æ€§æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                fixes = NetworkDiagnostics.apply_network_fixes()
                if fixes:
                    logger.info(f"åº”ç”¨äº†ç½‘ç»œä¿®å¤: {fixes}")
            
            # æ£€æŸ¥ä»£ç†é—®é¢˜
            proxy_issues = NetworkDiagnostics.detect_proxy_issues()
            if proxy_issues:
                logger.warning(f"æ£€æµ‹åˆ°ä»£ç†é—®é¢˜: {proxy_issues}")
            
            # æ£€æŸ¥MCPæœåŠ¡çŠ¶æ€ï¼ˆä¿®å¤æ ¹æœ¬åŸå› #3ï¼‰
            mcp_healthy = self._check_mcp_service_health()
            if not mcp_healthy:
                logger.error("MCPæœåŠ¡ä¸å¯ç”¨ï¼Œè¿™å°†å½±å“AIåˆ†æåŠŸèƒ½")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"ç³»ç»Ÿå¥åº·ç›‘æ§å¤±è´¥: {e}")
            return False
    
    def _check_mcp_service_health(self, max_retries=3, retry_delay=2):
        """æ£€æŸ¥MCPæœåŠ¡å¥åº·çŠ¶æ€ï¼ˆä¿®å¤æ ¹æœ¬åŸå› #3ï¼‰"""
        import requests
        
        for attempt in range(max_retries):
            try:
                # æµ‹è¯•MCPæœåŠ¡æ ¹è·¯å¾„
                response = requests.get('http://localhost:5000/', timeout=5)
                if response.status_code == 200:
                    logger.info("MCPæœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
                    
                    # è¿›ä¸€æ­¥æµ‹è¯•å…³é”®ç«¯ç‚¹
                    mcp_api_key = os.getenv('MCP_API_KEY')
                    if mcp_api_key:
                        headers = {'x-api-key': mcp_api_key}
                        test_response = requests.get('http://localhost:5000/list_allowed_files', 
                                                   headers=headers, timeout=5)
                        if test_response.status_code == 200:
                            logger.info("MCPæœåŠ¡APIç«¯ç‚¹å¯ç”¨")
                            return True
                        else:
                            logger.warning(f"MCP APIç«¯ç‚¹å“åº”å¼‚å¸¸: {test_response.status_code}")
                    
                    return True
                else:
                    logger.warning(f"MCPæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"MCPæœåŠ¡è¿æ¥å¤±è´¥ (attempt {attempt + 1}/{max_retries}): {e}")
                
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
            
        logger.error("MCPæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥")
        return False
    
    def start_ai_analysis_system(self):
        """å¯åŠ¨AIåˆ†æç³»ç»Ÿ"""
        if not hasattr(self, 'ai_orchestrator') or not self.ai_orchestrator:
            logger.info("AI Orchestrator not initialized, skipping start")
            return
        
        try:
            # AIç¼–æ’å™¨ä¸éœ€è¦æ˜¾å¼å¯åŠ¨ï¼Œå®ƒæ˜¯åŸºäºè°ƒç”¨çš„æ¨¡å¼
            # éªŒè¯ç³»ç»ŸçŠ¶æ€
            status = self.ai_orchestrator.get_analysis_status()
            logger.info("AI Analysis System status verified")
            logger.info(f"Available tools: {status.get('available_tools', [])}")
            logger.info("AI Analysis System ready for use")
            
        except Exception as e:
            logger.error(f"Failed to start AI analysis system: {e}")
    
    def get_system_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Šï¼ˆä¿®å¤æ ¹æœ¬åŸå› #5ï¼‰"""
        status = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'system_components': {
                'data_manager': {
                    'available': hasattr(self, 'data_manager') and self.data_manager is not None,
                    'last_update': self.last_update.isoformat() if self.last_update else None
                },
                'ai_orchestrator': {
                    'available': hasattr(self, 'ai_orchestrator') and self.ai_orchestrator is not None,
                    'status': 'active' if hasattr(self, 'ai_orchestrator') and self.ai_orchestrator else 'inactive'
                },
                'mcp_service': {
                    'available': self._check_mcp_service_health(),
                    'url': 'http://localhost:5000'
                },
                'network': {
                    'connectivity': self._check_network_connectivity(),
                    'proxy_issues': self._get_proxy_status()
                }
            },
            'environment': {
                'api_keys_configured': self._check_api_keys(),
                'config_loaded': hasattr(self, 'data_manager') and self.data_manager.config is not None
            },
            'system_health': {
                'is_running': self.is_running,
                'overall_status': self._calculate_overall_status()
            }
        }
        
        # æ·»åŠ AIç³»ç»Ÿè¯¦æƒ…
        if hasattr(self, 'ai_orchestrator') and self.ai_orchestrator:
            try:
                ai_status = self.ai_orchestrator.get_analysis_status()
                status['system_components']['ai_orchestrator']['details'] = ai_status
            except Exception as e:
                status['system_components']['ai_orchestrator']['error'] = str(e)
        
        return status
    
    def _check_network_connectivity(self):
        """æ£€æŸ¥ç½‘ç»œè¿æ¥æ€§"""
        try:
            from src.account_fetcher import NetworkDiagnostics
            return NetworkDiagnostics.test_connectivity()
        except Exception:
            return False
    
    def _get_proxy_status(self):
        """è·å–ä»£ç†çŠ¶æ€"""
        try:
            from src.account_fetcher import NetworkDiagnostics
            issues = NetworkDiagnostics.detect_proxy_issues()
            return {'has_issues': len(issues) > 0, 'issues': issues}
        except Exception as e:
            return {'has_issues': True, 'error': str(e)}
    
    def _check_api_keys(self):
        """æ£€æŸ¥APIå¯†é’¥é…ç½®"""
        required_keys = ['OKX_API_KEY', 'OKX_API_SECRET', 'OKX_API_PASSPHRASE', 
                        'MCP_API_KEY', 'DASHSCOPE_API_KEY']
        configured_keys = [key for key in required_keys if os.getenv(key)]
        return {
            'total_required': len(required_keys),
            'configured': len(configured_keys),
            'missing': [key for key in required_keys if not os.getenv(key)],
            'all_configured': len(configured_keys) == len(required_keys)
        }
    
    def _calculate_overall_status(self):
        """è®¡ç®—ç³»ç»Ÿæ•´ä½“çŠ¶æ€"""
        issues = []
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶
        if not (hasattr(self, 'data_manager') and self.data_manager):
            issues.append('data_manager_unavailable')
        
        if not (hasattr(self, 'ai_orchestrator') and self.ai_orchestrator):
            issues.append('ai_orchestrator_unavailable')
        
        if not self._check_mcp_service_health():
            issues.append('mcp_service_unavailable')
        
        api_status = self._check_api_keys()
        if not api_status['all_configured']:
            issues.append('api_keys_missing')
        
        if not issues:
            return 'healthy'
        elif len(issues) <= 2:
            return 'degraded'
        else:
            return 'critical'
    
    def stop_ai_analysis_system(self):
        """åœæ­¢AIåˆ†æç³»ç»Ÿ"""
        try:
            # AIç¼–æ’å™¨ä¸éœ€è¦æ˜¾å¼åœæ­¢
            # è®°å½•åœæ­¢ä¿¡æ¯
            logger.info("AI Analysis System stopped")
            
        except Exception as e:
            logger.error(f"Failed to stop AI analysis system: {e}")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        logger.info("Starting scheduler...")
        self.is_running = True
        
        while self.is_running:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logger.info("Scheduler stopped by user")
                self.is_running = False
                break
            except Exception as e:
                logger.error(f"Scheduler error: {e}")
                time.sleep(60)
    
    def run_once(self):
        """è¿è¡Œä¸€æ¬¡æ•°æ®è·å–"""
        logger.info("Running single data fetch...")
        return self.fetch_all_data()

def run_mcp_service():
    """è¿è¡ŒMCPæœåŠ¡"""
    import uvicorn
    logger.info("Starting MCP service on port 5000...")
    
    # ç¡®ä¿MCP APIå¯†é’¥å·²è®¾ç½®
    if not os.getenv('MCP_API_KEY'):
        logger.error("MCP_API_KEY not set!")
        return
    
    try:
        uvicorn.run(
            "main:mcp_app", 
            host="0.0.0.0", 
            port=5000, 
            log_level="info",
            reload=False
        )
    except Exception as e:
        logger.error(f"MCP service error: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("AI Trading System Starting...")
    print("=" * 60)
    
    trading_system = None
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        trading_system = AITradingSystem()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        trading_system.setup_scheduler()
        
        # å¯åŠ¨AIåˆ†æç³»ç»Ÿï¼ˆè®¾è®¡æ–‡æ¡£è¦æ±‚çš„Orchestratoræ¨¡å¼ï¼‰
        print("\n[AI] å¯åŠ¨AIåˆ†æç³»ç»Ÿ...")
        try:
            trading_system.start_ai_analysis_system()
            print("âœ… [AI] AIåˆ†æç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        except Exception as ai_error:
            print(f"âŒ [AI] AIåˆ†æç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {ai_error}")
            logger.error(f"AIåˆ†æç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {ai_error}")
            # ç»§ç»­è¿è¡Œï¼Œä½†æ ‡è®°AIåŠŸèƒ½ä¸å¯ç”¨
        
        # æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥ï¼ˆä¿®å¤æ ¹æœ¬åŸå› #5ï¼‰
        print("\n[HEALTH] æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        health_status = trading_system._monitor_system_health()
        if health_status:
            print("âœ… [HEALTH] ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
        else:
            print("âš ï¸ [HEALTH] ç³»ç»Ÿå¥åº·æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        
        # ç«‹å³è¿è¡Œä¸€æ¬¡æ•°æ®è·å–
        print("\n[DATA] æ‰§è¡Œåˆå§‹æ•°æ®è·å–...")
        initial_results = trading_system.run_once()
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        successful_timeframes = len(initial_results.get('success', []))
        failed_timeframes = len(initial_results.get('failed', []))
        print(f"[SUCCESS] æˆåŠŸå¤„ç† {successful_timeframes} ä¸ªæ—¶é—´å‘¨æœŸ")
        
        if failed_timeframes > 0:
            print(f"[ERROR] å¤±è´¥ {failed_timeframes} ä¸ªæ—¶é—´å‘¨æœŸ")
            for failed in initial_results.get('failed', []):
                print(f"   - {failed.get('timeframe')}: {failed.get('reason')}")
        
        # å¯åŠ¨MCPæœåŠ¡ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­ï¼‰
        print("\n[MCP] å¯åŠ¨MCPæœåŠ¡ (ç«¯å£ 5000)...")
        
        # åœ¨åå°è¿è¡ŒMCPæœåŠ¡å’Œå®šæ—¶ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=3) as executor:
            # å¯åŠ¨MCPæœåŠ¡ï¼ˆåå°çº¿ç¨‹ï¼‰
            mcp_future = executor.submit(run_mcp_service)
            
            # ç­‰å¾…MCPæœåŠ¡å¯åŠ¨å¹¶éªŒè¯å¥åº·çŠ¶æ€ï¼ˆä¿®å¤æ ¹æœ¬åŸå› #1ï¼‰
            print("[WAIT] ç­‰å¾…MCPæœåŠ¡å¯åŠ¨å¹¶éªŒè¯å°±ç»ª...")
            
            # é€æ­¥éªŒè¯MCPæœåŠ¡çŠ¶æ€ï¼Œæœ€å¤šç­‰å¾…30ç§’
            mcp_ready = False
            for wait_attempt in range(30):  # 30ç§’è¶…æ—¶
                time.sleep(1)
                if trading_system._check_mcp_service_health():
                    mcp_ready = True
                    print(f"[SUCCESS] MCPæœåŠ¡åœ¨{wait_attempt + 1}ç§’åå°±ç»ª")
                    break
                    
                # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (wait_attempt + 1) % 5 == 0:
                    print(f"[WAIT] ç­‰å¾…MCPæœåŠ¡... ({wait_attempt + 1}/30ç§’)")
            
            if not mcp_ready:
                print("[ERROR] MCPæœåŠ¡æœªèƒ½åœ¨é¢„æœŸæ—¶é—´å†…å¯åŠ¨")
                logger.error("MCPæœåŠ¡å¯åŠ¨è¶…æ—¶ï¼Œç³»ç»Ÿæ— æ³•æ­£å¸¸è¿è¡Œ")
                return
            
            print("[READY] ç³»ç»Ÿå·²å°±ç»ª - AIç°åœ¨å¯ä»¥é€šè¿‡MCPè®¿é—®æ•°æ®")
            
            # æ‰§è¡Œå¾…å¤„ç†çš„AIåˆ†æè¯·æ±‚ï¼ˆä¿®å¤æ ¹æœ¬åŸå› #4ï¼‰
            print("[AI] æ£€æŸ¥å¹¶æ‰§è¡Œå¾…å¤„ç†çš„AIåˆ†æ...")
            try:
                trading_system._execute_pending_analysis()
            except Exception as analysis_error:
                print(f"[ERROR] AIåˆ†ææ‰§è¡Œå¤±è´¥: {analysis_error}")
                logger.error(f"AIåˆ†ææ‰§è¡Œå¤±è´¥: {analysis_error}")
                # ä¸ç»ˆæ­¢ç³»ç»Ÿï¼Œç»§ç»­è¿è¡Œå®šæ—¶ä»»åŠ¡
            
            print("\n" + "=" * 60)
            print("ç³»ç»Ÿæ­£åœ¨è¿è¡Œ...")
            print("æŒ‰ Ctrl+C åœæ­¢")
            print("=" * 60)
            
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼ˆä¸»çº¿ç¨‹ï¼‰
            trading_system.start_scheduler()
        
    except KeyboardInterrupt:
        print("\n\n[STOP] ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    except Exception as e:
        print(f"\n[ERROR] ç³»ç»Ÿé”™è¯¯: {e}")
        logger.error(f"System error: {e}")
    finally:
        # åœæ­¢AIåˆ†æç³»ç»Ÿ
        if trading_system:
            try:
                trading_system.stop_ai_analysis_system()
            except:
                pass
        print("\n[EXIT] AIäº¤æ˜“ç³»ç»Ÿå·²åœæ­¢")

if __name__ == "__main__":
    main()
